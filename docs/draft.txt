Основа:

статьи

В статье "A Closer Look at Few-shot Classification" приведено сравнение двух типов алгоритмов Baseline/Baseline++ и Meta-Learning.

Алгоритмы Baseline/Baseline++, состоят из следующих шагов:
1) Разделить датасет на две части BaseSet и NovelSet, так, что множество меток примеров из BaseSet и NovelSet не пересекаются
2) Training stage: Обучаем веса Backbone net и Classifer на всех примерах из BaseSet. (Нужно ли делать валидацию на части примеров из BaseSet?)
3) Fine-tuning stage: Семплируем из NovelSet n классов, k примеров для каждого класса(Это множество называется support set), и q примеров для каждого класса(множество queryset). Фиксируем веса Backbonenet и обучаем Classifier на support set, полученный классификатор проверяем на queryset, репортим mean accuracy и std

Алгоритмы Meta-Learning:
1) Разделить датасет на две части BaseSet и NovelSet, так, что множество меток примеров из BaseSet и NovelSet не пересекаются
2) Разделяем BaseSet на TrainSet и ValSet, так, что множество меток примеров из BaseSet и NovelSet не пересекаются
3) Обучаем эпизод на BaseSet, валидируемся на ValSet
4) Meta-testing set аналогично

Вопрос: как тренировать backbone сетку?


