## Введение
Документ основан на статьях:

1. [A Closer Look at Few-shot Classification](https://openreview.net/pdf?id=HkxLXnAcFQ)
1. [Dense Classification and Implanting for Few-Shot Learning](https://arxiv.org/pdf/1903.05050.pdf)
1. [Matching Networks for One Shot Learning](https://arxiv.org/pdf/1606.04080.pdf)

## Описание алгоритмов
В статье [1] рассматриваются два типа алгоритмов: Baseline/Baseline++ и Meta-learning. 
Для работы требуется разделить датасет на два **Base** и **Novel**, так, что множество классов в Base и Novel не пересекаются. Далее обучение происходит на Base датасете, а тестирование на Novel. Так же можно в качестве Base и Novel использовать множества примеров из разных датасетов.

Алгоритмы настраивают классисификатор над вложениями(эмбединги) которые выдает базовая сетка. В статьях базовая сетка называется **Basenet** или **BackboneNet**. Классификатор на вложениях называют **Classifier** . 

Предполагается, что работа будет состоять из следующих этапов:
1. Обучение базовой сетки
1. Обучение классификатора на Base датасете(**Training stage** для Baseline/Baseline++ и **Meta-learning stage**  для Meta-learning)
1. Тестирование получившегося алгоритма на Novel датасете(**Fine-tuning stage** для Baseline/Baseline++ и **Meta-testing stage** для Meta-learning)

### Обучение базовой сетки
Про обучение базовой сетки ничего не нашел в данных статьях. Возможны следующие сценарии:
1. Инициализация сетки случайными весами
1. Использовать предобученные параметры. Здесь надо удостоверится, что мы не используем веса полученные на ImageNet, и, затем тестируемся на tinyImagenet.
1. Обучить сетку на Base датасете как классификатор. Обучение базовой сетки нужно провести на TrainSet, предварительно разделив его на Train и **Validation**, при этом данные множества могут пересекаться по классам.
В результате получаем веса для сетки.

### Training stage
На этой стадии выход базовой сетки соединяется с классификатором, далее происходит обучение по эпохам(**epoch**) на Base датасете. Обучаются либо все, либо часть весов базовой сетки. В данном случае можно использовать валидационный датасет.
В результате этой стадии получаем веса обученной сети для FewShot.

### Meta-learning state
На данной стадии для кроссвалидации нужен датасет. Можно получить его одним из следующих способов:
1. Разделить Base датасет на Train и **ValidationFewShot**, так чтобы эти множества не пересекались по классам. В дальнейшем для обучения используем только Train
1. В качестве валидации использовать другой датасет

В meta-learning обучается классификационная модель(**classification model**). Обучение происходит по эпизодам(**episode**). Эпизод для N-way k-shot состоит из следующих шагов:
1. Выбрать N классов из Base датасета
1. Для каждого класса выбирается k примеров, которые формируют **support set**
1. Для каждого класса выбирается q примеров, которые формируют **query set**

В каждом эпизоде обучаем веса базовой сетки и классификационную модель на support set, минимизируя функцию потерь на query set. Наконец, выбирается сетка, которая дает наиболее высокую точность на ValidationFewShot.
 В результате получаем веса обученной сети для FewShot и, возможно, параметры классификационной модели.

### Fine-tuning/Meta-testing stage
На этой стадии происходит замер средней точности и стандартного отклонения на некотором количестве эпизодов, данные для эпизодов берутся из Novel датасета.
 Из статей не ясно:
 1. Нужно ли изменять веса базовой сетки во время обучения в эпизоде Baseline/Baseline++ модели?
 1. Нужно ли "сбрасывать" параметры классификатора/классификационной модели между эпизодами?

## Архитектура
Структура классов:
![](img/Main.jpg)
